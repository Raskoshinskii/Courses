{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP/GCS Configuration for Mage\n",
    "In order to use Mage with GCP we have to configure our GCP first and provide credentials for Mage. \n",
    "\n",
    "1. Create Google Cloud Storage Bucket (GCS Bucket)\n",
    "\n",
    "    1.1 Go to GCP -> Cloud Storage -> Buckets and create a new bucket (select default options)\n",
    "\n",
    "2. Create a new *service account* for Mage\n",
    "\n",
    "    2.1 Go to *Service Accounts* -> *Create* -> Role: Owner\n",
    "\n",
    "    2.2 Select the created account and go to *Keys* -> *Add Key* \n",
    "\n",
    "    2.3 Move downloaded GCP Key into Mage project directory\n",
    "\n",
    "3. Test connection to GCP Bucket\n",
    "\n",
    "    3.1 Open Mage and got to *Files*\n",
    "\n",
    "    3.1 Since the directory from which *docker-compose* is running is mounted to Mage, Mage can access all the files from that *dir*\n",
    "\n",
    "    3.2 Go to *test_config* block and select *BigQuery* -> *Profile: Default* -> test connection\n",
    "\n",
    "4. Move `titanic_clean.csv` into GCP bucket and query that data from Mage\n",
    "\n",
    "    4.1 Open GCP bucket and drug `titanic_clean.csv` from VSCode into GCP\n",
    "\n",
    "    4.2 Create a new block *test_gcs*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Writing Data to GCS Using Mage\n",
    "1. Create a new *batch pipeline* and copy previous blocks: \n",
    "- *load_api_data*\n",
    "- *transform_taxi_data*\n",
    "\n",
    "2. Define a new block `taxidata_to_gcs_parquet`\n",
    "\n",
    "    2.1 Configure the block and run. Check in GCP that the data has been successfully saved.\n",
    "\n",
    "3. Define a new block `taxi_to_gcs_partitioned_parquet`\n",
    "\n",
    "    3.1 create *generic data exporter*\n",
    "\n",
    "    3.2 link it with clean data\n",
    "\n",
    "    3.3 import pyarrow, os into the block\n",
    "\n",
    "    3.4 set up env variables called `GOOGLE_APPLICATION_CREDENTIALS`\n",
    "\n",
    "    3.5 define `bucket_name`, `project_id` and `table_name`\n",
    "\n",
    "    3.6 define root_path -> `root_path = f'{bucket_name}/{table_name}'`\n",
    "\n",
    "    3.7 define `export_data` function\n",
    "    \n",
    "    3.7.1 create `tpep_pickup_date` column\n",
    "\n",
    "    3.7.2 define pyarrow table and GCS file system\n",
    "\n",
    "    3.7.3 define pq.write_to_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### GCS to BigQuery\n",
    "1. Define the following steps in a new Pipeline:\n",
    "- `load_taxi_gcs`: read/load file/data from GCS\n",
    "- `transform_staged_data`: some transformation for the dataset\n",
    "- `write_taxi_to_bigquery` : move data to Google BigQuery\n",
    "\n",
    "2. Check BigQuery\n",
    "\n",
    "3. Define Schedule Workflow\n",
    "\n",
    "    3.1 Go to Triggers and select desired Trigger Type -> Schedule\n",
    "\n",
    "\n",
    "### Parameterized Execution\n",
    "...\n",
    "\n",
    "### Notes\n",
    "- Large files a not saved in a single file, such files are partitioned.\n",
    "- Usually large files are partitioned by day. It guarantees even distribution of data and easy management when reading."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
