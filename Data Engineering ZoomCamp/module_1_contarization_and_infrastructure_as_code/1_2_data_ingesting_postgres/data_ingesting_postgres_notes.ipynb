{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting NY Taxi Data using Postgres\n",
    "[Lesson Video](https://www.youtube.com/watch?v=2JM-ziJt0WI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n",
    "\n",
    "### Running Postgres using Docker\n",
    "1. Pull Postgres image with the below configuration and create a Postgres container:\n",
    "```\n",
    "Windows WSL (volume path is different)\n",
    "\n",
    "    docker run -it \\\n",
    "        -e POSTGRES_USER='root' \\ # -e describes Environment variables \n",
    "        -e POSTGRES_PASSWORD='root' \\\n",
    "        -e POSTGRES_DB=\"ny_taxi\" \\ # dataset name is a good name for the DB name\n",
    "        -v c/home/vlad/dev/Data Engineering ZoomCamp/Module 1 - Containerization Infrastructure as Code/Data Ingesting with Postgres/pg_db_volume:/var/lib/postgresql/data \\ # mountaing the volume to a container\n",
    "        --name postgres_db \\ # container name\n",
    "        -p 5432:5432 \\  # port mapping \n",
    "        postgres:13\n",
    "\n",
    "Linux\n",
    "\n",
    "    docker run -it \\\n",
    "        -e POSTGRES_USER='root' \\\n",
    "        -e POSTGRES_PASSWORD='root' \\\n",
    "        -e POSTGRES_DB=\"ny_taxi\" \\ \n",
    "        -v $(pwd)/pg_db_volume:/var/lib/postgresql/data \\ # mountaing the volume to a container\n",
    "        -- name postgres_db\n",
    "        -p 5432:5432 \\  # port mapping\n",
    "        postgres:13\n",
    "```\n",
    "\n",
    "Sometimes the formatted code from above doesn't work -> use one line command:\n",
    "    - `docker run -it -e POSTGRES_USER='root' -e POSTGRES_PASSWORD='root' -e POSTGRES_DB='ny_taxi' -v $(pwd)/pg_db_volume:/var/lib/postgresql/data --name postgres_db -p 5432:5432 postgres:13`\n",
    "\n",
    "If the container already exists, run:\n",
    "    - `docker start postgres_db`\n",
    "    - Even if we drop the container, db remains saved because of Docker Volume\n",
    "\n",
    "### Using `pgcli` for Postgres Connection\n",
    "1. Install `pgcli` in current Python environment and connect to the Postgres using another terminal\n",
    "pgcli installation:\n",
    "    - `pip install --upgrade pip`\n",
    "    - `pip install \"psycopg[binary,pool]\"`\n",
    "    - `pip install pgcli`\n",
    "\n",
    "2. Connect to postgres container using `pgcli` in a new terminal:\n",
    "    - `pgcli -h localhost -p 5432 -u root -d ny_taxi` -> test connection to Postgres NY Taxi DB (docker)\n",
    "    - `\\dt` -> test the connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NY Taxi Data\n",
    "1. Download the dataset from [here](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and save in dir `data/`\n",
    "    - mkdir `data`, `cd data`, `wget <dataset_url>`\n",
    "\n",
    "Currently all dataset in NY Taxi data is in parquet format. We need to transform it into `.csv`\n",
    "\n",
    "\n",
    "### NY Taxi Data Ingesting\n",
    "1. Prepare the data for ingesting to Postgres DB:\n",
    "    - Open Jupyter Lab in a new terminal: `jupyter lab`\n",
    "    - Install dependencies: `pip install pandas sqlalchemy pyarrow`\n",
    "\n",
    "2. Create script to transform data from parquet into CSV and make the transformation\n",
    "\n",
    "3. Create Schema for Postgres \n",
    "    - read csv data (only first 100 rows for now)\n",
    "    - check that all columns in a DataFrame have valid data types, transform if needed\n",
    "    - use pandas io module to generate the schema from table:\n",
    "         - `print(pd.io.sql.get_schema(data, name='yellow_taxi_data'))`\n",
    "\n",
    "4. Create Connection object to Postgres using Pandas\n",
    "    - `engine = create_engine(f'postgresql://{user_name}:{pwd}@localhost:5432/ny_taxi')`\n",
    "    - generate Postgres related schema: `print(pd.io.sql.get_schema(data, name='yellow_taxi_data', con=engine))`\n",
    "\n",
    "\n",
    "### Dataset Batching\n",
    "Since the dataset is big, we will injest the data in batches to prevent issues with data base.\n",
    "\n",
    "- Create iterator object to batch the DataFrame:\n",
    "    - `df_iterator = pd.read_csv('yellow_tripdata_2023-01.csv', iterator=True, chunksize=100000)`\n",
    "\n",
    "- Get the first batch and generate schema for it:\n",
    "    - `df_curent_batch = next(df_iterator)`\n",
    "\n",
    "- Create table in Postgres using pd.to_sql():\n",
    "    - `df_curent_batch.head(n=0).to_sql(name=pg_table_name, con=engine, if_exists='replace')` -> replace True to create a table\n",
    "\n",
    "- Append the first chunk of data to Postgres\n",
    "    - `df_curent_batch.to_sql(name=pg_table_name, con=engine, if_exists='append')`\n",
    "\n",
    "- Validate that the first batch has been moved to Postges using pgcli\n",
    "    - `SELECT COUNT(*) FROM yellow_taxi_data`\n",
    "    \n",
    "- Create script to automate the process completely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Postgres Data using pgAdmin \n",
    "As an alternative to `pgcli` pgAdmin can be used. Pull the docker image of pgAdmin using the following script:\n",
    "```\n",
    "docker run -it \\\n",
    "    -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n",
    "    -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n",
    "    -p 8080:80 \\ \n",
    "    dpage/pgadmin4\n",
    "```\n",
    "\n",
    "However, running the command above will not allow accessing Postgres container. This is because pgAdmin and Postgres are **running in different containers and cannot access each other.** **Solution: put those containers in the same network**\n",
    "\n",
    "1. Create a docker network\n",
    "    - `docker network create <name>`\n",
    "\n",
    "2. Remove previous docker container and redefine it. Run docker command from volume directory, redirect there\n",
    "```\n",
    "docker run -it \\\n",
    "    -e POSTGRES_USER='root' \\\n",
    "    -e POSTGRES_PASSWORD='root' \\\n",
    "    -e POSTGRES_DB=\"ny_taxi\" \\ \n",
    "    -v $(pwd)/pg_db_volume:/var/lib/postgresql/data \\ \n",
    "    -p 5432:5432 \\ \n",
    "    --network pg_network \\\n",
    "    --name postgres_db \\\n",
    "    postgres:13\n",
    "```\n",
    "- `docker run -it -e POSTGRES_USER='root' -e POSTGRES_PASSWORD='root' -e POSTGRES_DB=\"ny_taxi\" -v $(pwd)/pg_db_volume:/var/lib/postgresql/data -p 5432:5432 --network pg_network --name postgres_db postgres:13`\n",
    "\n",
    "3. Run pgAdmin in the same network\n",
    "```\n",
    "docker run -d \\\n",
    "  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n",
    "  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n",
    "  -p 8080:80 \\\n",
    "  --network=pg_network \\\n",
    "  --name pgadmin \\\n",
    "  dpage/pgadmin4\n",
    "```\n",
    "- `docker run -d -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" -e PGADMIN_DEFAULT_PASSWORD=\"root\" -p 8080:80 --network=pg_network --name pgadmin dpage/pgadmin4`\n",
    "\n",
    "4. Once pgAdmin is open, create a new server and check that the ingested data exists\n",
    "    - host name/address: <container_name> / (e.g. postgres_db)\n",
    "    - port 5432\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Into a Script/Pipeline\n",
    "To process the data that we downloaded in a more robust way, we have to migrate all the scripts from notebook into a Python file. Jupyter notebook can be easily converted using the following command:\n",
    "- `jupyter nbconvert --to FORMAT notebook.ipynb`\n",
    "\n",
    "1. Transform notebook into Python script using `jupyter nbconvert`, clean it.\n",
    "\n",
    "2. Use `argparse` library to configure the script\n",
    "    2.1 Define what parameters must be configured (e.g. pg_user_name, pg_password, etc)\n",
    "    2.2 Create any functions needed for the pipeline and put them all into `if __name__ == '__main__':`\n",
    "    2.3 Drop the table in Postgres: `DROP TABLE <table_name>`\n",
    "    2.4 Run the following script to run your pipeline:\n",
    "    ```\n",
    "    URL=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "        python run_injesting_pipeline.py \\\n",
    "            --user=root \\\n",
    "            --password=root \\\n",
    "            --host=localhost \\\n",
    "            --port=5432 \\\n",
    "            --db_name=ny_taxi \\\n",
    "            --pg_table_name=yellow_taxi_trips \\\n",
    "            --url=${URL} \\\n",
    "            --n_rows_read=1500000 \\\n",
    "            --chunksize=100000 \\\n",
    "    ```\n",
    "\n",
    "### Pipeline/Script Dockerization\n",
    "Now let's dockerise our pipeline using Docker\n",
    "1. Create Dockerfile for your script\n",
    "2. Create a container\n",
    "    - `docker build -t pg_taxi_injesting:0.1 .`\n",
    "\n",
    "We mount volume to a container to preserve the downloaded data. No need to download each time after restarting.\n",
    "\n",
    "3. Check that the container has been built correctly, run the container:\n",
    "```\n",
    "URL=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "docker run -it \\\n",
    "    -v $(pwd)/data:/app/data \\\n",
    "    --name pg_taxi_injesting \\\n",
    "    --network=pg_network \\\n",
    "    pg_taxi_injesting:0.1 \\\n",
    "        --user=root \\\n",
    "        --password=root \\\n",
    "        --host=postgres_db \\\n",
    "        --port=5432 \\\n",
    "        --db_name=ny_taxi \\\n",
    "        --pg_table_name=yellow_taxi_trips \\\n",
    "        --url=${URL} \\\n",
    "        --n_rows_read=1500000 \\\n",
    "        --chunksize=100000 \n",
    "```\n",
    "\n",
    "### Notes\n",
    "- passing passwords is unsecure in bash commands -> history is saved\n",
    "- the safest way -> pass through environment variables\n",
    "- to fix several lines of code -> `Shift + -> or <-` and then `Ctr + Alt + down`\n",
    "- check datatypes in argparse, especially int values!\n",
    "- volumes better to mount instead of defining in Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
