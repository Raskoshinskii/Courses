Week_1
- Что такое машинное обучение?
- Основные виды обучение в МО
- Интерполяция
- Основная задача обучения с учителем
- Объект, ответ, признак, признаковое описание
- Определение алгоритма МО
- Функционал ошибки
- Постановка задачи обучения в МО (обучение с учителем)
- Нотация Айверсона
- Ключевые вопросы в МО
- Основные типы задач МО (обучение с учителем)
- Основные типы задач МО (обучение без учителем)
- Основные типы признаков в МО
- Выбросы, в чем проблема?
- Проблема редких значений категориальных признаков
- Проблема скошенности вещественных признаков
- Какие вопросы мы должны обязательно задать при решении задачи МО (например регрессии)?
- Формула модели линейной регрессии
- Какую функцию ошибки минимизируем и почему?
  * Почему мы возводим отклонения в квадрат?
- Задача обучения линейной регрессии в матричном виде
- Аналитическая формула нахождения весов ЛР
- Недостаток аналитического подхода
  * Какая сложность обращения матрицы в аналитическом решение?
- Алгоритм градиентного спуска в ЛР (инициализация весов, формула)
- Какие критерии останова градиентного спуска могут быть определены?
- Что такое парная регрессия?
- Как можно выбрать шаг градиентного спуска?
- Стохастический градиентный спуск (формула)
- Отличия между SGD и GD
- Достоинства SGD
- Что такое онлайн обучение?
- Что такое линейный классификатор (формула)?
- Геометрический смысл ЛК (гиперплоскость, расстояние от объекта до гиперплоскости)
- Что такое отступ? О чем говорит знак отступа и модуль отступа?
- Функция потерь для задачи бинарной классификации
  * В чем основная проблема оптимизации функции потерь, используя отступ? Как можно решить данную проблему?
  * Что такое Zero-one Loss, Hinge Loss и Logistic Loss?
  * Можно ли использовать Zero-one Loss для мультиклассовой классификации?
  * Почему нельзя использовать любые значения для кодировки классов в бинарной классификации? 
    * Какие значения обычно используются?
 * Основные отличия между: LogLoss, Categorical Cross-Entropy, Sparse Categorical Cross Entropy, Weighted LogLoss и Focal Loss?
Week_2
- Переобучение, не дообучение и способы выявления переобучения
- В чем основная проблема мультиколлинеарности для линейных алгоритмов?
- Опасна ли мультиколлинеарность для нелинейных алгоритмов?
- Что такое регуляризация?
- L2 и L1 регуляризация
  * В чем основные отличия?
  * Что контролирует параметр лямбда?
  * Чему эквивалентна регуляризация?
- Методы оценки качества алгоритмов (отложенная выборка, много ОВ, кросс-валидация, достоинства и недостатки)
  * Зависит ли K - число фолдов в кросс-валидации от размера данных?
  * В каких случаях наблюдения не стоит перемешивать?
- Что такое гиперпараметры модели?
- Как следует осуществлять выбор лучших алгоритмов и гиперпараметров
- Основные метрики качества в задачах регрессии (достоинства/недостатки)
  * Почему MSE может настроиться на выбросы?
- Может ли R^2 быть отрицательным, что это означает?
- Пример несимметричной метрики качества в задаче регрессии
- Вероятностный смысл метрик в задачах регрессии
- Основное отличие метрик в задачах регрессии и классификации?
- Чем плоха "обычная точность" в классификации?
- Основные метрики классификации, матрица ошибок, интерпретация Accuracy, Precision, Recall
  * Как еще называют Recall и почему?
  * Что такое Specificity?
- Что такое ошибки первого и второго рода?
- Интерпретация F-меры..Зачем она нужна?
- Формула F-меры и расширенной F-меры. Интерпретация коэффициета бетта
  * Как балансировать важность между Precision и Recall?
- Зачем нужно оценивать принадлежность к классу?
- Что такое и как строится PR Curve?
  * В каких пределах изменяется?
  * PR-AUC
  * Чему соответствует случайно выбранная точка на кривой PR AUC?
- Что такое ROC Curve и как строится?
  * В каких пределах изменяется?
  * ROC-AUC
  * Что характеризует линия на графике ROC Curve, проходящая от нуля до 1?
- Определения TPR, FPR, чувствительности и специфичности
- Можно ли одновременно повышать чувствительность и специфичность классификатора?
- PR-AUC и ROC-AUC их чувствительность к балансу классов?
Week_3
- Что такое метод максимального правдоподобия (опиши постановку задачи)?
- Что такое функция правдоподобия?
- Полезные свойства ММП
- Что такое функции семейства дивергенции Брегмана. Какая оценка обеспечивает минимум для таких ФП?
- Что является лучшей линейной аппроксимацией для среднеабсолютной ошибки и квантильной ошибки?
- Как еще можно бороться с переобучением?
- Формулы для Ridge и Lasso Regression
- Следует ли использовать единичный признак (вес w0) в регуляризаторе?
- Как изменяются значения весов для МНК, Ridge и Lasso регрессии?
- График зависимости весов от Y для МНК, Ridge и Lasso регрессии (основное отличие)
- Из чего складывается математическое ожидание квадрата ошибки регрессии? (декомпозиция ошибки)
- Графики баланса между смещением и дисперсией (Bias-Variance Trade off)
- Каковы значения смещения и дисперсии для модели переобученной и с регуляризацией?
- Аналитическое решение для Ridge и Lasso Regression
  *  Метод LDA. Что это такое?
  * Основная цель метода LDA?
  * Основные задачи метода LDA?
  * Что такое дискриминантные признаки, дискриминирующая функция?
  * Как проводится гиперплоскость между классами в LDA?
  * Основные +/- LDA
  * Является ли LDA полноценным классификатором?
- Как работает Линейный Дискриминант Фишера?
- Логистическая регрессия 
  * Что такое логистическая регрессия? (бинарная классификация)
  * Что предсказываем в логистической регрессии?
  * Как перейти от вещественных предсказаний линейной модели к вероятности?
  * Почему мы используем именно сигмоиду для оценки вероятности к положительному классу а не какую-нибудь другую функцию?
  * Что такое логит преобразование?
  * Как обучается логистическая регрессия?
  * Как меняется функция потерь в зависимости от меток класса?
  * Почему лучше использовать LogLoss, а не МНК в качестве функции потерь?
  * К максимизации чего приводит минимизация логистической функции потерь?
  * LogLoss и кросс-энтропия это одно и тоже?
  * LogLoss и Logistic Function (Sigmoid) одно и тоже?
  * Что будет если метки класса идеально разделимы линейно?
  * Регуляризация для логистической регрессии
  * Как связана однослойная нейронная сеть и логистическая регрессия?
  * Какая будет предсказана вероятность моделью для объекта на котором скалярное произведение равно 0?
- Масштабирования признаков
  * К каким проблемам приводит неотмасштабированные признаки?
  * Методы масштабирования признаков, их сравнение, +/-
- Понятие спрямляющего пространства (примеры)
- Какие признаки можно добавить в линейную модель чтобы повысить пространство?
- К какой проблеме может привести создание новых признаков?
- К чему приводит логарифмирование распределения таргета?
- В чем основное отличие категориальных признаков от порядковых?
- Как кодируют категориальные признаки для моделей?
- Когда выборку можно считать несбалансированной?
- Какой класс будет предсказывать классификатор, если классы несбалансированны?
- Основные методы борьбы с дисбалансом классов (что является гиперпараметром в этих методах)?
- Каким методом можно решить задачу много классовой классификации?
- Какие метрики используются для задач много классовой классификации и как их рассчитать?
- Как предсказывается класс в задаче мультиклассовой классификации
Week_4
- Дерево Решений
  * Краткое определение, как влияет тип задачи на значения в листьях?
  * К какому классу моделей относится ДР?
  * Как строят ДР в МО и почему?
  * Как может быть определен ответ в листе ДР?
  * Что мы пытаемся минимизировать при разбиении вершины?
  * Формула критерия ошибки для разбиения вершины (что представляет из себя каждый параметр такой ошибки)
  * Основные критерии останова и стрижка ДР
  * Основной минус стрижки деревьев?
- Критерии Информативности 
  * Критерии информативности для регрессии и классификации (критерий Джини и Энтропийный критерий)
  * Когда достигается оптимум критерия Джини и Энтропии?
  * Интерпретация энтропийного критерия и критерия Джини
  * В чем отличие критерия Джини (Gini Impurity) от индекса Джини (Gini Index)
  * Прирост информации (Information Gain), зачем нужен?
- Композиция алгоритмов
  * Что такое композиция алгоритмов?
  * Суть Бутсрэпа и зачем он используется в композиции деревьев?
  * На какие параметры обычно раскладывается ошибка моделей?
  * Что такое шум, смещение и разброс
  * Каким смещением и разбросом обладают:
    * Линейные модели, решающие деревья и композиции алгоритмов?
  * Каким основным свойством должны обладать алгоритмы композиций (ensembles)
  * Основные методы снижения зависимости между базовыми алгоритмами?
- Случайный Лес
  * Алгоритм построения случайного леса
  * Каким способом снижают корреляцию между деревьями в алгоритме случайного леса?
  * Rule of Thumb для определения числа признаков в подмножестве признаков (регрессия и классификация)?
  * В чем суть подхода "Out of Bag"?
  * Можно ли оценивать важность признаков при помощи подхода "Out-of-Bag"?
  * Почему случайный лес применим не ко всем задачам. Что делать в этом случае?
  * Почему не рекомендуется строить деревья небольшой глубины в RandomForest?
  * Почему RandomForest называется Random?
- Алгоритм градиентного бустинга
  * Основное отличие GradientBoosting от RandomForest?
  * Что такое градиентный бустинг, алгоритм?
  * Почему первый алгоритм в композиции должный быть несложным?
  * Зачем нужно вычислять градиент в данном алгоритме?
  * Какими способами может быть проинициализирован самый первый алгоритм (классификация, регрессия)?
  * Описать как происходит уменьшение ошибки в бустинге (напримере MSE, формула в общем виде)
  * Как происходит построение нового дерева/алгоритма?
  * Что приближает (чему эквивалентен) новый алгоритм?
  * Чему эквивалентен алгоритм градиентного бустинга?
  * Основная причина переобучения ГБ?
  * Как бороться с переобучением в ГБ?
  * Как можно выбирать шаг в градиентном бустинге?
  * Основные гиперпараметры градиентного бустинга? 
  * Какой метод получим если применим бутстреп при построении базовых алгоритмов?
  * Зачем применять бутстреп и шаг градиентного спуска в градиентном бустинге?
  * На какую ошибку настраивается структура дерева в градиентном бустинге?
  * Почему GradientBoosting называется градиентный?
  * Сколько обычно деревьев может быть использовано в бустинге (в среднем)?
Week_5
- Что такое однослойная нейронная сеть? (объяснение + нарисовать)
- Какие требования предъявляются к функции активации?
- За что отвечает вес w0 (bias) в НС?
- Основные функции активации (сигмоидная, гипербалический тангенс, softsign, Relu)
- Почему необходимы многослойные НС?
- Основной метод обучения НС, его +/-
- Задача оптимизации НС и основные параметры сети
- Какими методами может быть осуществлена оптимизация НС, их +/-
- Какой метод регуляризации в НС может быть использован?
- Как определить какие нейроны можно выключить при обучении?
- Зачем нужно строить кривые обучения для НС?
- Обязательно ли функция активации должна быть дифференцируемой?
- Зачем использовать softmax?
Week_6
- Наивный Байесовский Классификатор
  * В чем заключается суть байесовкской классификации?
  * Пример наивной байесовской классификации на примере спам фильтра
  * Правильный алгоритм наивного байесовского классификатора и КАКУЮ вероятность мы оцениваем?
  * Как обучается наивный баес и сложности которые возникают/могут возникнуть?
  * Какими способами может быть оценена P(x|y)?
- Что такое метрические алгоритмы?
- Банальное определение метрики?
- Примеры метрик (три хватит) как определить лучшую?
- Проблема проклятия размерности
  * Как связаны между собой число наблюдений и число признаков? (разреженность)
    * Случай когда число наблюдений меньше числа признаков
    * Случай когда число наблюдений больше числа признаков
    * Случай когда число наблюдений и число признаков совпадают
  * Как влияет проклятие размерности на расстояния между объектами в многомерных пространствах?
  * Как бороться с проклятием размерности?
- KNN
  * Интуиция метода KNN, взвешенный KNN
  * Можно ли при помощи KNN решать задачу регрессии, как решаем?
  * Основные параметры KNN как их подбирать?
  * Есть ли у KNN обучаемые параметры?
- SVM
  * Что представляет собой алгоритм SVM (2 основных элемента метода)
  * Что такое разделяющие полосы, и как их определить?
  * Что такое ширина разделяющей полосы? Как она задается?
  * Интуиция SVM
  * Что максимизирует SVM (линейно разделимая и неразделимая выборки)
  * SVM это линейная модель?
  * Kernel Trick, зачем использовать?